{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 14:02:26.033400 140550374143744 deprecation_wrapper.py:119] From /home/gusleo_bali/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorlayer/layers.py:31: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0913 14:02:26.037894 140550374143744 deprecation_wrapper.py:119] From /home/gusleo_bali/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorlayer/layers.py:3576: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import os, time, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "Data augmentation used to enrich original data from Brats2017. The augmentation implement to each data is below:\n",
    "1. Flip (left right)\n",
    "2. Elastic Tranform\n",
    "3. Rotate\n",
    "4. Shift\n",
    "5. Shear\n",
    "6. Zoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort_imgs(data):\n",
    "    \"\"\" data augumentation \"\"\"\n",
    "    x1, x2, x3, x4, y = data\n",
    "    # x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],  # previous without this, hard-dice=83.7\n",
    "    #                         axis=0, is_random=True) # up down\n",
    "    x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],\n",
    "                            axis=1, is_random=True) # left right\n",
    "    x1, x2, x3, x4, y = tl.prepro.elastic_transform_multi([x1, x2, x3, x4, y],\n",
    "                            alpha=720, sigma=24, is_random=True)\n",
    "    x1, x2, x3, x4, y = tl.prepro.rotation_multi([x1, x2, x3, x4, y], rg=20,\n",
    "                            is_random=True, fill_mode='constant') # nearest, constant\n",
    "    x1, x2, x3, x4, y = tl.prepro.shift_multi([x1, x2, x3, x4, y], wrg=0.10,\n",
    "                            hrg=0.10, is_random=True, fill_mode='constant')\n",
    "    x1, x2, x3, x4, y = tl.prepro.shear_multi([x1, x2, x3, x4, y], 0.05,\n",
    "                            is_random=True, fill_mode='constant')\n",
    "    x1, x2, x3, x4, y = tl.prepro.zoom_multi([x1, x2, x3, x4, y],\n",
    "                            zoom_range=[0.9, 1.1], is_random=True,\n",
    "                            fill_mode='constant')\n",
    "    return x1, x2, x3, x4, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs(X, y, path, show=False):\n",
    "    \"\"\" show one slice \"\"\"\n",
    "    if y.ndim == 2:\n",
    "        y = y[:,:,np.newaxis]\n",
    "    assert X.ndim == 3\n",
    "    tl.visualize.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
    "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
    "        X[:,:,3,np.newaxis], y]), size=(1, 5),\n",
    "        image_path=path)\n",
    "    if(show):\n",
    "        tl.visualize.read_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs2(X, y_, y, path, show=False):\n",
    "    \"\"\" show one slice with target \"\"\"\n",
    "    if y.ndim == 2:\n",
    "        y = y[:,:,np.newaxis]\n",
    "    if y_.ndim == 2:\n",
    "        y_ = y_[:,:,np.newaxis]\n",
    "    assert X.ndim == 3\n",
    "    tl.visualize.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
    "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
    "        X[:,:,3,np.newaxis], y_, y]), size=(1, 6),\n",
    "        image_path=path)\n",
    "    if(show):\n",
    "        tl.visualize.read_image(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder to save trained model and result images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] checkpoint exists ...\n",
      "[!] samples/all exists ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"all\"\n",
    "save_dir = \"checkpoint\"\n",
    "tl.files.exists_or_mkdir(save_dir)\n",
    "tl.files.exists_or_mkdir(\"samples/{}\".format(task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA\n",
    "Data from Brats2017 and loaded from prepare_data_with_valid.py\n",
    "Data importing from Brats2017 refers to DATA_SIZE define in prepate_data_with_valid which is (\"all\", \"half\", \"small\")\n",
    "1. X_train_input: contain the training data from 4 types MRI scan (flair, t1, t1-c and t2)\n",
    "2. X_train_target: contain the training data from segmentation of tumors\n",
    "3. X_dev_input: containing the validation data from 4 types MRI scan\n",
    "4. x_dev_target: containing the validation data from segementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:00<00:00, 171604.31it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival Data Count 163\n",
      "Data Size: half\n",
      "Data training used for HGG: 100 and LGG: 30\n",
      "Survival patient for HGG: 77 and LGG: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:54<00:00, 28.61s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flair': {'mean': 44.627055070478356, 'std': 125.00108363401826}, 't1': {'mean': 70.65435360404604, 'std': 215.5494205932192}, 't1ce': {'mean': 81.44920319823545, 'std': 276.57440203992627}, 't2': {'mean': 79.98272799748415, 'std': 240.54336376267653}}\n",
      "Preparing image for HGG Validation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:32<00:00,  1.07s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing image for LGG Validation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.08s/it]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing image for HGG Train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:31<00:00,  1.48it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing image for LGG Train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import prepare_data_with_valid as dataset\n",
    "X_train = dataset.X_train_input\n",
    "y_train = dataset.X_train_target[:,:,:,np.newaxis]\n",
    "X_test = dataset.X_dev_input\n",
    "y_test = dataset.X_dev_target[:,:,:,np.newaxis]\n",
    "\n",
    "\n",
    "if task == 'all':\n",
    "    y_train = (y_train > 0).astype(int)\n",
    "    y_test = (y_test > 0).astype(int)\n",
    "elif task == 'necrotic':\n",
    "    y_train = (y_train == 1).astype(int)\n",
    "    y_test = (y_test == 1).astype(int)\n",
    "elif task == 'edema':\n",
    "    y_train = (y_train == 2).astype(int)\n",
    "    y_test = (y_test == 2).astype(int)\n",
    "elif task == 'enhance':\n",
    "    y_train = (y_train == 4).astype(int)\n",
    "    y_test = (y_test == 4).astype(int)\n",
    "else:\n",
    "    exit(\"Unknow task %s\" % task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE HYPER-PARAMETERS\n",
    "1. Batch Size: refers to the number of training examples utilized in one iteration\n",
    "2. Learning Rate (lr): learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.\n",
    "3. Beta1: The exponential decay rate for the 1st moment estimates on Adam Optimizer \n",
    "4. Epoch: refers to the number of iteration to train all the training dataset \n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "This network using Adam Optimizer. Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "lr = 0.0001 \n",
    "# lr_decay = 0.5\n",
    "# decay_every = 100\n",
    "beta1 = 0.9\n",
    "n_epoch = 5\n",
    "print_freq_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### SHOWING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0913 14:07:17.868486 140550374143744 deprecation_wrapper.py:119] From /home/gusleo_bali/u-net-brain-tumor/model.py:10: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0913 14:07:17.869851 140550374143744 deprecation_wrapper.py:119] From /home/gusleo_bali/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorlayer/layers.py:262: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "W0913 14:07:17.870762 140550374143744 deprecation_wrapper.py:119] From /home/gusleo_bali/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorlayer/layers.py:1280: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] InputLayer  u_net/inputs: (10, 240, 240, 4)\n",
      "  [TL] Conv2dLayer u_net/conv1_1: shape:[3, 3, 4, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv2_1: shape:[3, 3, 64, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv3_1: shape:[3, 3, 128, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool3: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv4_1: shape:[3, 3, 256, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool4: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv5_1: shape:[3, 3, 512, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv5_2: shape:[3, 3, 1024, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv4: shape:[3, 3, 512, 1024] out_shape:[10, 30, 30, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat4: 1024\n",
      "  [TL] Conv2dLayer u_net/uconv4_1: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv3: shape:[3, 3, 256, 512] out_shape:[10, 60, 60, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat3: 512\n",
      "  [TL] Conv2dLayer u_net/uconv3_1: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv2: shape:[3, 3, 128, 256] out_shape:[10, 120, 120, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat2: 256\n",
      "  [TL] Conv2dLayer u_net/uconv2_1: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv1: shape:[3, 3, 64, 128] out_shape:[10, 240, 240, 64] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat1: 128\n",
      "  [TL] Conv2dLayer u_net/uconv1_1: shape:[3, 3, 128, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1: shape:[1, 1, 64, 1] strides:[1, 1, 1, 1] pad:SAME act:sigmoid\n",
      "  [TL] InputLayer  u_net/inputs: (10, 240, 240, 4)\n",
      "  [TL] Conv2dLayer u_net/conv1_1: shape:[3, 3, 4, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv2_1: shape:[3, 3, 64, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv3_1: shape:[3, 3, 128, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool3: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv4_1: shape:[3, 3, 256, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool4: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv5_1: shape:[3, 3, 512, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv5_2: shape:[3, 3, 1024, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv4: shape:[3, 3, 512, 1024] out_shape:[10, 30, 30, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat4: 1024\n",
      "  [TL] Conv2dLayer u_net/uconv4_1: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv3: shape:[3, 3, 256, 512] out_shape:[10, 60, 60, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat3: 512\n",
      "  [TL] Conv2dLayer u_net/uconv3_1: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv2: shape:[3, 3, 128, 256] out_shape:[10, 120, 120, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat2: 256\n",
      "  [TL] Conv2dLayer u_net/uconv2_1: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0913 14:07:18.388545 140550374143744 deprecation_wrapper.py:119] From /home/gusleo_bali/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorlayer/layers.py:172: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] DeConv2dLayer u_net/deconv1: shape:[3, 3, 64, 128] out_shape:[10, 240, 240, 64] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat1: 128\n",
      "  [TL] Conv2dLayer u_net/uconv1_1: shape:[3, 3, 128, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1: shape:[1, 1, 64, 1] strides:[1, 1, 1, 1] pad:SAME act:sigmoid\n",
      "  [*] geting variables with u_net\n",
      "  got   0: u_net/conv1_1/W_conv2d:0   (3, 3, 4, 64)\n",
      "  got   1: u_net/conv1_1/b_conv2d:0   (64,)\n",
      "  got   2: u_net/conv1_2/W_conv2d:0   (3, 3, 64, 64)\n",
      "  got   3: u_net/conv1_2/b_conv2d:0   (64,)\n",
      "  got   4: u_net/conv2_1/W_conv2d:0   (3, 3, 64, 128)\n",
      "  got   5: u_net/conv2_1/b_conv2d:0   (128,)\n",
      "  got   6: u_net/conv2_2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got   7: u_net/conv2_2/b_conv2d:0   (128,)\n",
      "  got   8: u_net/conv3_1/W_conv2d:0   (3, 3, 128, 256)\n",
      "  got   9: u_net/conv3_1/b_conv2d:0   (256,)\n",
      "  got  10: u_net/conv3_2/W_conv2d:0   (3, 3, 256, 256)\n",
      "  got  11: u_net/conv3_2/b_conv2d:0   (256,)\n",
      "  got  12: u_net/conv4_1/W_conv2d:0   (3, 3, 256, 512)\n",
      "  got  13: u_net/conv4_1/b_conv2d:0   (512,)\n",
      "  got  14: u_net/conv4_2/W_conv2d:0   (3, 3, 512, 512)\n",
      "  got  15: u_net/conv4_2/b_conv2d:0   (512,)\n",
      "  got  16: u_net/conv5_1/W_conv2d:0   (3, 3, 512, 1024)\n",
      "  got  17: u_net/conv5_1/b_conv2d:0   (1024,)\n",
      "  got  18: u_net/conv5_2/W_conv2d:0   (3, 3, 1024, 1024)\n",
      "  got  19: u_net/conv5_2/b_conv2d:0   (1024,)\n",
      "  got  20: u_net/deconv4/W_deconv2d:0   (3, 3, 512, 1024)\n",
      "  got  21: u_net/deconv4/b_deconv2d:0   (512,)\n",
      "  got  22: u_net/uconv4_1/W_conv2d:0   (3, 3, 1024, 512)\n",
      "  got  23: u_net/uconv4_1/b_conv2d:0   (512,)\n",
      "  got  24: u_net/uconv4_2/W_conv2d:0   (3, 3, 512, 512)\n",
      "  got  25: u_net/uconv4_2/b_conv2d:0   (512,)\n",
      "  got  26: u_net/deconv3/W_deconv2d:0   (3, 3, 256, 512)\n",
      "  got  27: u_net/deconv3/b_deconv2d:0   (256,)\n",
      "  got  28: u_net/uconv3_1/W_conv2d:0   (3, 3, 512, 256)\n",
      "  got  29: u_net/uconv3_1/b_conv2d:0   (256,)\n",
      "  got  30: u_net/uconv3_2/W_conv2d:0   (3, 3, 256, 256)\n",
      "  got  31: u_net/uconv3_2/b_conv2d:0   (256,)\n",
      "  got  32: u_net/deconv2/W_deconv2d:0   (3, 3, 128, 256)\n",
      "  got  33: u_net/deconv2/b_deconv2d:0   (128,)\n",
      "  got  34: u_net/uconv2_1/W_conv2d:0   (3, 3, 256, 128)\n",
      "  got  35: u_net/uconv2_1/b_conv2d:0   (128,)\n",
      "  got  36: u_net/uconv2_2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got  37: u_net/uconv2_2/b_conv2d:0   (128,)\n",
      "  got  38: u_net/deconv1/W_deconv2d:0   (3, 3, 64, 128)\n",
      "  got  39: u_net/deconv1/b_deconv2d:0   (64,)\n",
      "  got  40: u_net/uconv1_1/W_conv2d:0   (3, 3, 128, 64)\n",
      "  got  41: u_net/uconv1_1/b_conv2d:0   (64,)\n",
      "  got  42: u_net/uconv1_2/W_conv2d:0   (3, 3, 64, 64)\n",
      "  got  43: u_net/uconv1_2/b_conv2d:0   (64,)\n",
      "  got  44: u_net/uconv1/W_conv2d:0   (1, 1, 64, 1)\n",
      "  got  45: u_net/uconv1/b_conv2d:0   (1,)\n",
      "[*] Load checkpoint/u_net_all.npz SUCCESS!\n",
      "start training\n",
      "Epoch 0 step 100 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.572958s (2d with distortion)\n",
      "Epoch 0 step 200 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.592963s (2d with distortion)\n",
      "Epoch 0 step 300 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.575724s (2d with distortion)\n",
      "Epoch 0 step 400 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.584883s (2d with distortion)\n",
      "Epoch 0 step 500 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.574233s (2d with distortion)\n",
      "Epoch 0 step 600 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.575402s (2d with distortion)\n",
      "Epoch 0 step 700 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.567722s (2d with distortion)\n",
      "Epoch 0 step 800 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.571007s (2d with distortion)\n",
      "Epoch 0 step 900 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.545063s (2d with distortion)\n",
      "Epoch 0 step 1000 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.554970s (2d with distortion)\n",
      " ** Epoch [0/5] train 1-dice: 0.994220 hard-dice: 0.005780 iou: 0.005780 took 1654.828760s (2d with distortion)\n",
      " **                 test 1-dice: 0.993548 hard-dice: 0.006452 iou: 0.006452 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "Epoch 1 step 100 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.586669s (2d with distortion)\n",
      "Epoch 1 step 200 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.556476s (2d with distortion)\n",
      "Epoch 1 step 300 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.580702s (2d with distortion)\n",
      "Epoch 1 step 400 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.563093s (2d with distortion)\n",
      "Epoch 1 step 500 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.566661s (2d with distortion)\n",
      "Epoch 1 step 600 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.567474s (2d with distortion)\n",
      "Epoch 1 step 700 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.563137s (2d with distortion)\n",
      "Epoch 1 step 800 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.575355s (2d with distortion)\n",
      "Epoch 1 step 900 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.566262s (2d with distortion)\n",
      "Epoch 1 step 1000 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.586773s (2d with distortion)\n",
      " ** Epoch [1/5] train 1-dice: 0.993256 hard-dice: 0.006744 iou: 0.006744 took 1636.656949s (2d with distortion)\n",
      " **                 test 1-dice: 0.995161 hard-dice: 0.004839 iou: 0.004839 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "Epoch 2 step 100 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.569181s (2d with distortion)\n",
      "Epoch 2 step 200 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.565959s (2d with distortion)\n",
      "Epoch 2 step 300 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.561920s (2d with distortion)\n",
      "Epoch 2 step 400 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.555507s (2d with distortion)\n",
      "Epoch 2 step 500 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.562003s (2d with distortion)\n",
      "Epoch 2 step 600 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.564455s (2d with distortion)\n",
      "Epoch 2 step 700 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.574503s (2d with distortion)\n",
      "Epoch 2 step 800 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.588238s (2d with distortion)\n",
      "Epoch 2 step 900 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.576026s (2d with distortion)\n",
      "Epoch 2 step 1000 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.581243s (2d with distortion)\n",
      " ** Epoch [2/5] train 1-dice: 0.995183 hard-dice: 0.004817 iou: 0.004817 took 1634.274230s (2d with distortion)\n",
      " **                 test 1-dice: 0.993548 hard-dice: 0.006452 iou: 0.006452 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "Epoch 3 step 100 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.565849s (2d with distortion)\n",
      "Epoch 3 step 200 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.547628s (2d with distortion)\n",
      "Epoch 3 step 300 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.567204s (2d with distortion)\n",
      "Epoch 3 step 400 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.560998s (2d with distortion)\n",
      "Epoch 3 step 500 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.537731s (2d with distortion)\n",
      "Epoch 3 step 600 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.565442s (2d with distortion)\n",
      "Epoch 3 step 700 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.555453s (2d with distortion)\n",
      "Epoch 3 step 800 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.613614s (2d with distortion)\n",
      "Epoch 3 step 900 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.595639s (2d with distortion)\n",
      "Epoch 3 step 1000 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.559988s (2d with distortion)\n",
      " ** Epoch [3/5] train 1-dice: 0.996146 hard-dice: 0.003854 iou: 0.003854 took 1633.192626s (2d with distortion)\n",
      " **                 test 1-dice: 0.991935 hard-dice: 0.008065 iou: 0.008065 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "Epoch 4 step 100 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.552975s (2d with distortion)\n",
      "Epoch 4 step 200 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.558206s (2d with distortion)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 300 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.574545s (2d with distortion)\n",
      "Epoch 4 step 400 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.568245s (2d with distortion)\n",
      "Epoch 4 step 500 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.575156s (2d with distortion)\n",
      "Epoch 4 step 600 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.572516s (2d with distortion)\n",
      "Epoch 4 step 700 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.568281s (2d with distortion)\n",
      "Epoch 4 step 800 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.569008s (2d with distortion)\n",
      "Epoch 4 step 900 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.580990s (2d with distortion)\n",
      "Epoch 4 step 1000 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.559108s (2d with distortion)\n",
      " ** Epoch [4/5] train 1-dice: 0.999037 hard-dice: 0.000963 iou: 0.000963 took 1629.521114s (2d with distortion)\n",
      " **                 test 1-dice: 0.991935 hard-dice: 0.008065 iou: 0.008065 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "Epoch 5 step 100 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.568899s (2d with distortion)\n",
      "Epoch 5 step 200 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.564009s (2d with distortion)\n",
      "Epoch 5 step 300 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.577975s (2d with distortion)\n",
      "Epoch 5 step 400 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.565880s (2d with distortion)\n",
      "Epoch 5 step 500 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.586290s (2d with distortion)\n",
      "Epoch 5 step 600 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.569708s (2d with distortion)\n",
      "Epoch 5 step 700 1-dice: 0.000000 hard-dice: 1.000000 iou: 1.000000 took 1.574228s (2d with distortion)\n",
      "Epoch 5 step 800 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.552853s (2d with distortion)\n",
      "Epoch 5 step 900 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.558364s (2d with distortion)\n",
      "Epoch 5 step 1000 1-dice: 1.000000 hard-dice: 0.000000 iou: 0.000000 took 1.586990s (2d with distortion)\n",
      " ** Epoch [5/5] train 1-dice: 0.994220 hard-dice: 0.005780 iou: 0.005780 took 1639.501320s (2d with distortion)\n",
      " **                 test 1-dice: 0.993548 hard-dice: 0.006452 iou: 0.006452 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n"
     ]
    }
   ],
   "source": [
    "showImage = True\n",
    "\n",
    "# show one slice\n",
    "X = np.asarray(X_train[80])\n",
    "y = np.asarray(y_train[80])\n",
    "# print(X.shape, X.min(), X.max()) # (240, 240, 4) -0.380588 2.62761\n",
    "# print(y.shape, y.min(), y.max()) # (240, 240, 1) 0 1\n",
    "nw, nh, nz = X.shape\n",
    "vis_imgs(X, y, 'samples/{}/_train_im.png'.format(task), showImage)\n",
    "# show data augumentation results\n",
    "for i in range(batch_size):\n",
    "    x_flair, x_t1, x_t1ce, x_t2, label = distort_imgs([X[:,:,0,np.newaxis], X[:,:,1,np.newaxis],\n",
    "            X[:,:,2,np.newaxis], X[:,:,3,np.newaxis], y])#[:,:,np.newaxis]])\n",
    "    # print(x_flair.shape, x_t1.shape, x_t1ce.shape, x_t2.shape, label.shape) # (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1)\n",
    "    X_dis = np.concatenate((x_flair, x_t1, x_t1ce, x_t2), axis=2)\n",
    "    # print(X_dis.shape, X_dis.min(), X_dis.max()) # (240, 240, 4) -0.380588233471 2.62376139209\n",
    "    vis_imgs(X_dis, label, 'samples/{}/_train_im_aug{}.png'.format(task, i), showImage)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    with tf.device('/gpu:0'): #<- remove it if you train on CPU or other GPU\n",
    "        ###======================== DEFIINE MODEL =======================###\n",
    "        ## nz is 4 as we input all Flair, T1, T1c and T2.\n",
    "        t_image = tf.placeholder('float32', [batch_size, nw, nh, nz], name='input_image')\n",
    "        ## labels are either 0 or 1\n",
    "        t_seg = tf.placeholder('float32', [batch_size, nw, nh, 1], name='target_segment')\n",
    "        ## train inference\n",
    "        net = model.u_net(t_image, is_train=True, reuse=False, n_out=1)\n",
    "        ## test inference\n",
    "        net_test = model.u_net(t_image, is_train=False, reuse=True, n_out=1)\n",
    "\n",
    "        ###======================== DEFINE LOSS =========================###\n",
    "        ## train losses\n",
    "        out_seg = net.outputs\n",
    "        dice_loss = 1 - tl.cost.dice_coe(out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
    "        iou_loss = tl.cost.iou_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
    "        dice_hard = tl.cost.dice_hard_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
    "        loss = dice_loss\n",
    "\n",
    "        ## test losses\n",
    "        test_out_seg = net_test.outputs\n",
    "        test_dice_loss = 1 - tl.cost.dice_coe(test_out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
    "        test_iou_loss = tl.cost.iou_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
    "        test_dice_hard = tl.cost.dice_hard_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
    "\n",
    "    ###======================== DEFINE TRAIN OPTS =======================###\n",
    "    t_vars = tl.layers.get_variables_with_name('u_net', True, True)\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope('learning_rate'):\n",
    "            lr_v = tf.Variable(lr, trainable=False)\n",
    "        train_op = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(loss, var_list=t_vars)\n",
    "\n",
    "    ###======================== LOAD MODEL ==============================###\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    ## load existing model if possible\n",
    "    tl.files.load_and_assign_npz(sess=sess, name=save_dir+'/u_net_{}.npz'.format(task), network=net)\n",
    "    print(\"start training\")\n",
    "    ###======================== TRAINING ================================###\n",
    "    for epoch in range(0, n_epoch+1):\n",
    "        epoch_time = time.time()\n",
    "        ## update decay learning rate at the beginning of a epoch\n",
    "        # if epoch !=0 and (epoch % decay_every == 0):\n",
    "        #     new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "        #     sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
    "        #     log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "        #     print(log)\n",
    "        # elif epoch == 0:\n",
    "        #     sess.run(tf.assign(lr_v, lr))\n",
    "        #     log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "        #     print(log)\n",
    "\n",
    "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
    "        for batch in tl.iterate.minibatches(inputs=X_train, targets=y_train,\n",
    "                                    batch_size=batch_size, shuffle=True):\n",
    "            images, labels = batch\n",
    "            step_time = time.time()\n",
    "            ## data augumentation for a batch of Flair, T1, T1c, T2 images\n",
    "            # and label maps synchronously.\n",
    "            data = tl.prepro.threading_data([_ for _ in zip(images[:,:,:,0, np.newaxis],\n",
    "                    images[:,:,:,1, np.newaxis], images[:,:,:,2, np.newaxis],\n",
    "                    images[:,:,:,3, np.newaxis], labels)],\n",
    "                    fn=distort_imgs) # (10, 5, 240, 240, 1)\n",
    "            b_images = data[:,0:4,:,:,:]  # (10, 4, 240, 240, 1)\n",
    "            b_labels = data[:,4,:,:,:]\n",
    "            b_images = b_images.transpose((0,2,3,1,4))\n",
    "            b_images.shape = (batch_size, nw, nh, nz)\n",
    "\n",
    "            ## update network\n",
    "            _, _dice, _iou, _diceh, out = sess.run([train_op,\n",
    "                    dice_loss, iou_loss, dice_hard, net.outputs],\n",
    "                    {t_image: b_images, t_seg: b_labels})\n",
    "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
    "            n_batch += 1\n",
    "\n",
    "            ## you can show the predition here:\n",
    "            # vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_tmp.png\".format(task))\n",
    "            # exit()\n",
    "\n",
    "            # if _dice == 1: # DEBUG\n",
    "            #     print(\"DEBUG\")\n",
    "            #     vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_debug.png\".format(task))\n",
    "\n",
    "            if n_batch % print_freq_step == 0:\n",
    "                print(\"Epoch %d step %d 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\"\n",
    "                % (epoch, n_batch, _dice, _diceh, _iou, time.time()-step_time))\n",
    "\n",
    "            ## check model fail\n",
    "            if np.isnan(_dice):\n",
    "                exit(\" ** NaN loss found during training, stop training\")\n",
    "            if np.isnan(out).any():\n",
    "                exit(\" ** NaN found in output images during training, stop training\")\n",
    "\n",
    "        print(\" ** Epoch [%d/%d] train 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\" %\n",
    "                (epoch, n_epoch, total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch, time.time()-epoch_time))\n",
    "\n",
    "        ## save a predition of training set\n",
    "        for i in range(batch_size):\n",
    "            if np.max(b_images[i]) > 0:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch), showImage)\n",
    "                break\n",
    "            elif i == batch_size-1:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch), showImage)\n",
    "\n",
    "        ###======================== EVALUATION ==========================###\n",
    "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
    "        for batch in tl.iterate.minibatches(inputs=X_test, targets=y_test,\n",
    "                                        batch_size=batch_size, shuffle=True):\n",
    "            b_images, b_labels = batch\n",
    "            _dice, _iou, _diceh, out = sess.run([test_dice_loss,\n",
    "                    test_iou_loss, test_dice_hard, net_test.outputs],\n",
    "                    {t_image: b_images, t_seg: b_labels})\n",
    "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
    "            n_batch += 1\n",
    "\n",
    "        print(\" **\"+\" \"*17+\"test 1-dice: %f hard-dice: %f iou: %f (2d no distortion)\" %\n",
    "                (total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch))\n",
    "        print(\" task: {}\".format(task))\n",
    "        ## save a predition of test set\n",
    "        for i in range(batch_size):\n",
    "            if np.max(b_images[i]) > 0:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
    "                break\n",
    "            elif i == batch_size-1:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
    "\n",
    "        ###======================== SAVE MODEL ==========================###\n",
    "        tl.files.save_npz(net.all_params, name=save_dir+'/u_net_{}.npz'.format(task), sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
