{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import os, time, model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorlayer==1.5.4\n",
    "#!pip install tensorflow==1.10.0\n",
    "#!conda create -n env=unet python==3.5.6\n",
    "#!pip install nibabel\n",
    "#!pip install tqdm\n",
    "#!pip install scipy==0.18.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "Data augmentation used to enrich original data from Brats2017. The augmentation implement to each data is below:\n",
    "1. Flip (left right)\n",
    "2. Elastic Tranform\n",
    "3. Rotate\n",
    "4. Shift\n",
    "5. Shear\n",
    "6. Zoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort_imgs(data):\n",
    "    \"\"\" data augumentation \"\"\"\n",
    "    x1, x2, x3, x4, y = data\n",
    "    # x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],  # previous without this, hard-dice=83.7\n",
    "    #                         axis=0, is_random=True) # up down\n",
    "    x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],\n",
    "                            axis=1, is_random=True) # left right\n",
    "    x1, x2, x3, x4, y = tl.prepro.elastic_transform_multi([x1, x2, x3, x4, y],\n",
    "                            alpha=720, sigma=24, is_random=True)\n",
    "    x1, x2, x3, x4, y = tl.prepro.rotation_multi([x1, x2, x3, x4, y], rg=20,\n",
    "                            is_random=True, fill_mode='constant') # nearest, constant\n",
    "    x1, x2, x3, x4, y = tl.prepro.shift_multi([x1, x2, x3, x4, y], wrg=0.10,\n",
    "                            hrg=0.10, is_random=True, fill_mode='constant')\n",
    "    x1, x2, x3, x4, y = tl.prepro.shear_multi([x1, x2, x3, x4, y], 0.05,\n",
    "                            is_random=True, fill_mode='constant')\n",
    "    x1, x2, x3, x4, y = tl.prepro.zoom_multi([x1, x2, x3, x4, y],\n",
    "                            zoom_range=[0.9, 1.1], is_random=True,\n",
    "                            fill_mode='constant')\n",
    "    return x1, x2, x3, x4, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs(X, y, path, show=False):\n",
    "    \"\"\" show one slice \"\"\"\n",
    "    if y.ndim == 2:\n",
    "        y = y[:,:,np.newaxis]\n",
    "    assert X.ndim == 3\n",
    "    tl.visualize.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
    "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
    "        X[:,:,3,np.newaxis], y]), size=(1, 5),\n",
    "        image_path=path)\n",
    "    #if(show):\n",
    "        #tl.visualize.read_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs2(X, y_, y, path, show=False):\n",
    "    \"\"\" show one slice with target \"\"\"\n",
    "    if y.ndim == 2:\n",
    "        y = y[:,:,np.newaxis]\n",
    "    if y_.ndim == 2:\n",
    "        y_ = y_[:,:,np.newaxis]\n",
    "    assert X.ndim == 3\n",
    "    tl.visualize.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
    "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
    "        X[:,:,3,np.newaxis], y_, y]), size=(1, 6),\n",
    "        image_path=path)\n",
    "    #if(show):\n",
    "        #tl.visualize.read_image(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder to save trained model and result images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] checkpoint exists ...\n",
      "[!] samples/all exists ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"all\"\n",
    "save_dir = \"checkpoint\"\n",
    "tl.files.exists_or_mkdir(save_dir)\n",
    "tl.files.exists_or_mkdir(\"samples/{}\".format(task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA\n",
    "Data from Brats2017 and loaded from prepare_data_with_valid.py\n",
    "Data importing from Brats2017 refers to DATA_SIZE define in prepate_data_with_valid which is (\"all\", \"half\", \"small\")\n",
    "1. X_train_input: contain the training data from 4 types MRI scan (flair, t1, t1-c and t2)\n",
    "2. X_train_target: contain the training data from segmentation of tumors\n",
    "3. X_dev_input: containing the validation data from 4 types MRI scan\n",
    "4. x_dev_target: containing the validation data from segementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:00<00:00, 208627.27it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival Data Count 163\n",
      "Data Size: half\n",
      "Data training used for HGG: 100 and LGG: 30\n",
      "Survival patient for HGG: 77 and LGG: 0\n",
      "LOAD ALL IMAGES' PATH AND COMPUTE MEAN/ STD\n",
      "============================\n",
      "\n",
      "Load images type : flair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.18it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 11.30it/s]\n",
      "  2%|▏         | 2/100 [00:00<00:08, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images type : t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.65it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 11.70it/s]\n",
      "  2%|▏         | 2/100 [00:00<00:08, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images type : t1ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.66it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 11.45it/s]\n",
      "  2%|▏         | 2/100 [00:00<00:08, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images type : t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.54it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 11.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'t1': {'mean': 70.65435360404604, 'std': 215.5494205932192}, 't2': {'mean': 79.98272799748415, 'std': 240.54336376267653}, 'flair': {'mean': 44.627055070478356, 'std': 125.00108363401826}, 't1ce': {'mean': 81.44920319823545, 'std': 276.57440203992627}}\n",
      "Preparing image for HGG Validation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:24<00:00,  1.25it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing image for LGG Validation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.14it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing image for HGG Train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:35<00:00,  1.31it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing image for LGG Train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import prepare_data_with_valid as dataset\n",
    "X_train = dataset.X_train_input\n",
    "y_train = dataset.X_train_target[:,:,:,np.newaxis]\n",
    "X_test = dataset.X_dev_input\n",
    "y_test = dataset.X_dev_target[:,:,:,np.newaxis]\n",
    "\n",
    "\n",
    "if task == 'all':\n",
    "    y_train = (y_train > 0).astype(int)\n",
    "    y_test = (y_test > 0).astype(int)\n",
    "elif task == 'necrotic':\n",
    "    y_train = (y_train == 1).astype(int)\n",
    "    y_test = (y_test == 1).astype(int)\n",
    "elif task == 'edema':\n",
    "    y_train = (y_train == 2).astype(int)\n",
    "    y_test = (y_test == 2).astype(int)\n",
    "elif task == 'enhance':\n",
    "    y_train = (y_train == 4).astype(int)\n",
    "    y_test = (y_test == 4).astype(int)\n",
    "else:\n",
    "    exit(\"Unknow task %s\" % task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE HYPER-PARAMETERS\n",
    "1. Batch Size: refers to the number of training examples utilized in one iteration\n",
    "2. Learning Rate (lr): learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.\n",
    "3. Beta1: The exponential decay rate for the 1st moment estimates on Adam Optimizer \n",
    "4. Epoch: refers to the number of iteration to train all the training dataset \n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "This network using Adam Optimizer. Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "lr = 0.00001 \n",
    "# lr_decay = 0.5\n",
    "# decay_every = 100\n",
    "beta1 = 0.9\n",
    "n_epoch = 2\n",
    "print_freq_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### SHOWING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f4f8ac4e747b19096e1370c89a67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  [TL] InputLayer  u_net/inputs: (10, 240, 240, 4)\n",
      "  [TL] Conv2dLayer u_net/conv1_1: shape:[3, 3, 4, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv2_1: shape:[3, 3, 64, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv3_1: shape:[3, 3, 128, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool3: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv4_1: shape:[3, 3, 256, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool4: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv5_1: shape:[3, 3, 512, 1024] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv5_2: shape:[3, 3, 1024, 1024] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv4: shape:[3, 3, 512, 1024] out_shape:[10, 30, 30, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat4: 1024\n",
      "  [TL] Conv2dLayer u_net/uconv4_1: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv3: shape:[3, 3, 256, 512] out_shape:[10, 60, 60, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat3: 512\n",
      "  [TL] Conv2dLayer u_net/uconv3_1: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv2: shape:[3, 3, 128, 256] out_shape:[10, 120, 120, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat2: 256\n",
      "  [TL] Conv2dLayer u_net/uconv2_1: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv1: shape:[3, 3, 64, 128] out_shape:[10, 240, 240, 64] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat1: 128\n",
      "  [TL] Conv2dLayer u_net/uconv1_1: shape:[3, 3, 128, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv1: shape:[1, 1, 64, 1] strides:[1, 1, 1, 1] pad:SAME act:sigmoid\n",
      "  [TL] InputLayer  u_net/inputs: (10, 240, 240, 4)\n",
      "  [TL] Conv2dLayer u_net/conv1_1: shape:[3, 3, 4, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv2_1: shape:[3, 3, 64, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv3_1: shape:[3, 3, 128, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool3: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv4_1: shape:[3, 3, 256, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] PoolLayer   u_net/pool4: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv5_1: shape:[3, 3, 512, 1024] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/conv5_2: shape:[3, 3, 1024, 1024] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv4: shape:[3, 3, 512, 1024] out_shape:[10, 30, 30, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat4: 1024\n",
      "  [TL] Conv2dLayer u_net/uconv4_1: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv3: shape:[3, 3, 256, 512] out_shape:[10, 60, 60, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat3: 512\n",
      "  [TL] Conv2dLayer u_net/uconv3_1: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv2: shape:[3, 3, 128, 256] out_shape:[10, 120, 120, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat2: 256\n",
      "  [TL] Conv2dLayer u_net/uconv2_1: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] DeConv2dLayer u_net/deconv1: shape:[3, 3, 64, 128] out_shape:[10, 240, 240, 64] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat1: 128\n",
      "  [TL] Conv2dLayer u_net/uconv1_1: shape:[3, 3, 128, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:leaky_relu\n",
      "  [TL] Conv2dLayer u_net/uconv1: shape:[1, 1, 64, 1] strides:[1, 1, 1, 1] pad:SAME act:sigmoid\n",
      "  [*] geting variables with u_net\n",
      "  got   0: u_net/conv1_1/W_conv2d:0   (3, 3, 4, 64)\n",
      "  got   1: u_net/conv1_1/b_conv2d:0   (64,)\n",
      "  got   2: u_net/conv1_2/W_conv2d:0   (3, 3, 64, 64)\n",
      "  got   3: u_net/conv1_2/b_conv2d:0   (64,)\n",
      "  got   4: u_net/conv2_1/W_conv2d:0   (3, 3, 64, 128)\n",
      "  got   5: u_net/conv2_1/b_conv2d:0   (128,)\n",
      "  got   6: u_net/conv2_2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got   7: u_net/conv2_2/b_conv2d:0   (128,)\n",
      "  got   8: u_net/conv3_1/W_conv2d:0   (3, 3, 128, 256)\n",
      "  got   9: u_net/conv3_1/b_conv2d:0   (256,)\n",
      "  got  10: u_net/conv3_2/W_conv2d:0   (3, 3, 256, 256)\n",
      "  got  11: u_net/conv3_2/b_conv2d:0   (256,)\n",
      "  got  12: u_net/conv4_1/W_conv2d:0   (3, 3, 256, 512)\n",
      "  got  13: u_net/conv4_1/b_conv2d:0   (512,)\n",
      "  got  14: u_net/conv4_2/W_conv2d:0   (3, 3, 512, 512)\n",
      "  got  15: u_net/conv4_2/b_conv2d:0   (512,)\n",
      "  got  16: u_net/conv5_1/W_conv2d:0   (3, 3, 512, 1024)\n",
      "  got  17: u_net/conv5_1/b_conv2d:0   (1024,)\n",
      "  got  18: u_net/conv5_2/W_conv2d:0   (3, 3, 1024, 1024)\n",
      "  got  19: u_net/conv5_2/b_conv2d:0   (1024,)\n",
      "  got  20: u_net/deconv4/W_deconv2d:0   (3, 3, 512, 1024)\n",
      "  got  21: u_net/deconv4/b_deconv2d:0   (512,)\n",
      "  got  22: u_net/uconv4_1/W_conv2d:0   (3, 3, 1024, 512)\n",
      "  got  23: u_net/uconv4_1/b_conv2d:0   (512,)\n",
      "  got  24: u_net/uconv4_2/W_conv2d:0   (3, 3, 512, 512)\n",
      "  got  25: u_net/uconv4_2/b_conv2d:0   (512,)\n",
      "  got  26: u_net/deconv3/W_deconv2d:0   (3, 3, 256, 512)\n",
      "  got  27: u_net/deconv3/b_deconv2d:0   (256,)\n",
      "  got  28: u_net/uconv3_1/W_conv2d:0   (3, 3, 512, 256)\n",
      "  got  29: u_net/uconv3_1/b_conv2d:0   (256,)\n",
      "  got  30: u_net/uconv3_2/W_conv2d:0   (3, 3, 256, 256)\n",
      "  got  31: u_net/uconv3_2/b_conv2d:0   (256,)\n",
      "  got  32: u_net/deconv2/W_deconv2d:0   (3, 3, 128, 256)\n",
      "  got  33: u_net/deconv2/b_deconv2d:0   (128,)\n",
      "  got  34: u_net/uconv2_1/W_conv2d:0   (3, 3, 256, 128)\n",
      "  got  35: u_net/uconv2_1/b_conv2d:0   (128,)\n",
      "  got  36: u_net/uconv2_2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got  37: u_net/uconv2_2/b_conv2d:0   (128,)\n",
      "  got  38: u_net/deconv1/W_deconv2d:0   (3, 3, 64, 128)\n",
      "  got  39: u_net/deconv1/b_deconv2d:0   (64,)\n",
      "  got  40: u_net/uconv1_1/W_conv2d:0   (3, 3, 128, 64)\n",
      "  got  41: u_net/uconv1_1/b_conv2d:0   (64,)\n",
      "  got  42: u_net/uconv1_2/W_conv2d:0   (3, 3, 64, 64)\n",
      "  got  43: u_net/uconv1_2/b_conv2d:0   (64,)\n",
      "  got  44: u_net/uconv1/W_conv2d:0   (1, 1, 64, 1)\n",
      "  got  45: u_net/uconv1/b_conv2d:0   (1,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Load checkpoint/u_net_all.npz failed!\n",
      "start training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cc55e1731548d595d14ac13f9eb735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 100 1-dice: 0.971057 hard-dice: 0.145374 iou: 0.078385 took 1.762024s (2d with distortion)\n",
      "Epoch 0 step 200 1-dice: 0.981466 hard-dice: 0.056672 iou: 0.029162 took 1.774353s (2d with distortion)\n",
      "Epoch 0 step 300 1-dice: 0.626528 hard-dice: 0.383931 iou: 0.237571 took 1.761878s (2d with distortion)\n",
      "Epoch 0 step 400 1-dice: 0.467383 hard-dice: 0.489595 iou: 0.324148 took 1.757366s (2d with distortion)\n",
      "Epoch 0 step 500 1-dice: 0.627476 hard-dice: 0.349755 iou: 0.211941 took 1.774244s (2d with distortion)\n",
      "Epoch 0 step 600 1-dice: 0.283475 hard-dice: 0.686729 iou: 0.522915 took 1.745954s (2d with distortion)\n",
      "Epoch 0 step 700 1-dice: 0.319932 hard-dice: 0.643319 iou: 0.474186 took 1.770276s (2d with distortion)\n",
      "Epoch 0 step 800 1-dice: 0.176173 hard-dice: 0.785636 iou: 0.646952 took 1.772339s (2d with distortion)\n",
      "Epoch 0 step 900 1-dice: 0.256854 hard-dice: 0.693900 iou: 0.531276 took 1.758779s (2d with distortion)\n",
      "Epoch 0 step 1000 1-dice: 0.286439 hard-dice: 0.663680 iou: 0.496648 took 1.733819s (2d with distortion)\n",
      " ** Epoch [0/2] train 1-dice: 0.539791 hard-dice: 0.449889 iou: 0.329915 took 1859.447944s (2d with distortion)\n",
      " **                 test 1-dice: 0.290002 hard-dice: 0.669931 iou: 0.519547 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "Epoch 1 step 100 1-dice: 0.187237 hard-dice: 0.761427 iou: 0.614762 took 1.767548s (2d with distortion)\n",
      "Epoch 1 step 200 1-dice: 0.099306 hard-dice: 0.865108 iou: 0.762282 took 1.760142s (2d with distortion)\n",
      "Epoch 1 step 300 1-dice: 0.134195 hard-dice: 0.841479 iou: 0.726339 took 1.748028s (2d with distortion)\n",
      "Epoch 1 step 400 1-dice: 0.157074 hard-dice: 0.793836 iou: 0.658150 took 1.773582s (2d with distortion)\n",
      "Epoch 1 step 500 1-dice: 0.117213 hard-dice: 0.875240 iou: 0.778157 took 1.749475s (2d with distortion)\n",
      "Epoch 1 step 600 1-dice: 0.314588 hard-dice: 0.644999 iou: 0.476014 took 1.749997s (2d with distortion)\n",
      "Epoch 1 step 700 1-dice: 0.120617 hard-dice: 0.854450 iou: 0.745886 took 1.757667s (2d with distortion)\n",
      "Epoch 1 step 800 1-dice: 0.057251 hard-dice: 0.926108 iou: 0.862385 took 1.753879s (2d with distortion)\n",
      "Epoch 1 step 900 1-dice: 0.102966 hard-dice: 0.857869 iou: 0.751113 took 1.745723s (2d with distortion)\n",
      "Epoch 1 step 1000 1-dice: 0.319180 hard-dice: 0.628443 iou: 0.458197 took 1.763470s (2d with distortion)\n",
      " ** Epoch [1/2] train 1-dice: 0.234592 hard-dice: 0.724574 iou: 0.585936 took 1829.043463s (2d with distortion)\n",
      " **                 test 1-dice: 0.237048 hard-dice: 0.726624 iou: 0.589611 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "Epoch 2 step 100 1-dice: 0.197536 hard-dice: 0.736585 iou: 0.583012 took 1.733074s (2d with distortion)\n",
      "Epoch 2 step 200 1-dice: 0.835027 hard-dice: 0.146038 iou: 0.078771 took 1.749740s (2d with distortion)\n",
      "Epoch 2 step 300 1-dice: 0.177548 hard-dice: 0.761640 iou: 0.615040 took 1.738271s (2d with distortion)\n",
      "Epoch 2 step 400 1-dice: 0.212084 hard-dice: 0.736346 iou: 0.582711 took 1.770515s (2d with distortion)\n",
      "Epoch 2 step 500 1-dice: 0.142981 hard-dice: 0.828823 iou: 0.707684 took 1.742357s (2d with distortion)\n",
      "Epoch 2 step 600 1-dice: 0.091703 hard-dice: 0.879093 iou: 0.784269 took 1.751014s (2d with distortion)\n",
      "Epoch 2 step 700 1-dice: 0.070920 hard-dice: 0.898852 iou: 0.816286 took 1.759048s (2d with distortion)\n",
      "Epoch 2 step 800 1-dice: 0.244698 hard-dice: 0.695871 iou: 0.533591 took 1.744919s (2d with distortion)\n",
      "Epoch 2 step 900 1-dice: 0.230979 hard-dice: 0.718091 iou: 0.560173 took 1.757835s (2d with distortion)\n",
      "Epoch 2 step 1000 1-dice: 0.134840 hard-dice: 0.828472 iou: 0.707172 took 1.745224s (2d with distortion)\n",
      " ** Epoch [2/2] train 1-dice: 0.196188 hard-dice: 0.762847 iou: 0.633256 took 1819.621018s (2d with distortion)\n",
      " **                 test 1-dice: 0.235646 hard-dice: 0.729230 iou: 0.590469 (2d no distortion)\n",
      " task: all\n",
      "[*] checkpoint/u_net_all.npz saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showImage = True\n",
    "\n",
    "# show one slice\n",
    "X = np.asarray(X_train[80])\n",
    "y = np.asarray(y_train[80])\n",
    "# print(X.shape, X.min(), X.max()) # (240, 240, 4) -0.380588 2.62761\n",
    "# print(y.shape, y.min(), y.max()) # (240, 240, 1) 0 1\n",
    "nw, nh, nz = X.shape\n",
    "vis_imgs(X, y, 'samples/{}/_train_im.png'.format(task), showImage)\n",
    "# show data augumentation results\n",
    "for i in tqdm(range(batch_size)):\n",
    "    x_flair, x_t1, x_t1ce, x_t2, label = distort_imgs([X[:,:,0,np.newaxis], X[:,:,1,np.newaxis],\n",
    "            X[:,:,2,np.newaxis], X[:,:,3,np.newaxis], y])#[:,:,np.newaxis]])\n",
    "    # print(x_flair.shape, x_t1.shape, x_t1ce.shape, x_t2.shape, label.shape) # (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1)\n",
    "    X_dis = np.concatenate((x_flair, x_t1, x_t1ce, x_t2), axis=2)\n",
    "    # print(X_dis.shape, X_dis.min(), X_dis.max()) # (240, 240, 4) -0.380588233471 2.62376139209\n",
    "    #vis_imgs(X_dis, label, 'samples/{}/_train_im_aug{}.png'.format(task, i), showImage)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    with tf.device('/gpu:0'): #<- remove it if you train on CPU or other GPU\n",
    "        ###======================== DEFIINE MODEL =======================###\n",
    "        ## nz is 4 as we input all Flair, T1, T1c and T2.\n",
    "        t_image = tf.placeholder('float32', [batch_size, nw, nh, nz], name='input_image')\n",
    "        ## labels are either 0 or 1\n",
    "        t_seg = tf.placeholder('float32', [batch_size, nw, nh, 1], name='target_segment')\n",
    "        ## train inference\n",
    "        net = model.u_net(t_image, is_train=True, reuse=False, n_out=1)\n",
    "        ## test inference\n",
    "        net_test = model.u_net(t_image, is_train=False, reuse=True, n_out=1)\n",
    "\n",
    "        ###======================== DEFINE LOSS =========================###\n",
    "        ## train losses\n",
    "        out_seg = net.outputs\n",
    "        dice_loss = 1 - tl.cost.dice_coe(out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
    "        iou_loss = tl.cost.iou_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
    "        dice_hard = tl.cost.dice_hard_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
    "        loss = dice_loss\n",
    "\n",
    "        ## test losses\n",
    "        test_out_seg = net_test.outputs\n",
    "        test_dice_loss = 1 - tl.cost.dice_coe(test_out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
    "        test_iou_loss = tl.cost.iou_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
    "        test_dice_hard = tl.cost.dice_hard_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
    "\n",
    "    ###======================== DEFINE TRAIN OPTS =======================###\n",
    "    t_vars = tl.layers.get_variables_with_name('u_net', True, True)\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope('learning_rate'):\n",
    "            lr_v = tf.Variable(lr, trainable=False)\n",
    "        train_op = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(loss, var_list=t_vars)\n",
    "\n",
    "    ###======================== LOAD MODEL ==============================###\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    ## load existing model if possible\n",
    "    tl.files.load_and_assign_npz(sess=sess, name=save_dir+'/u_net_{}.npz'.format(task), network=net)\n",
    "    print(\"start training\")\n",
    "    ###======================== TRAINING ================================###\n",
    "    for epoch in tqdm(range(0, n_epoch+1)):\n",
    "        epoch_time = time.time()\n",
    "        ## update decay learning rate at the beginning of a epoch\n",
    "        # if epoch !=0 and (epoch % decay_every == 0):\n",
    "        #     new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "        #     sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
    "        #     log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "        #     print(log)\n",
    "        # elif epoch == 0:\n",
    "        #     sess.run(tf.assign(lr_v, lr))\n",
    "        #     log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "        #     print(log)\n",
    "\n",
    "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
    "        for batch in tl.iterate.minibatches(inputs=X_train, targets=y_train,\n",
    "                                    batch_size=batch_size, shuffle=True):\n",
    "            images, labels = batch\n",
    "            step_time = time.time()\n",
    "            ## data augumentation for a batch of Flair, T1, T1c, T2 images\n",
    "            # and label maps synchronously.\n",
    "            data = tl.prepro.threading_data([_ for _ in zip(images[:,:,:,0, np.newaxis],\n",
    "                    images[:,:,:,1, np.newaxis], images[:,:,:,2, np.newaxis],\n",
    "                    images[:,:,:,3, np.newaxis], labels)],\n",
    "                    fn=distort_imgs) # (10, 5, 240, 240, 1)\n",
    "            b_images = data[:,0:4,:,:,:]  # (10, 4, 240, 240, 1)\n",
    "            b_labels = data[:,4,:,:,:]\n",
    "            b_images = b_images.transpose((0,2,3,1,4))\n",
    "            b_images.shape = (batch_size, nw, nh, nz)\n",
    "\n",
    "            ## update network\n",
    "            _, _dice, _iou, _diceh, out = sess.run([train_op,\n",
    "                    dice_loss, iou_loss, dice_hard, net.outputs],\n",
    "                    {t_image: b_images, t_seg: b_labels})\n",
    "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
    "            n_batch += 1\n",
    "\n",
    "            ## you can show the predition here:\n",
    "            # vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_tmp.png\".format(task))\n",
    "            # exit()\n",
    "\n",
    "            # if _dice == 1: # DEBUG\n",
    "            #     print(\"DEBUG\")\n",
    "            #     vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_debug.png\".format(task))\n",
    "\n",
    "            if n_batch % print_freq_step == 0:\n",
    "                print(\"Epoch %d step %d 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\"\n",
    "                % (epoch, n_batch, _dice, _diceh, _iou, time.time()-step_time))\n",
    "\n",
    "            ## check model fail\n",
    "            if np.isnan(_dice):\n",
    "                exit(\" ** NaN loss found during training, stop training\")\n",
    "            if np.isnan(out).any():\n",
    "                exit(\" ** NaN found in output images during training, stop training\")\n",
    "\n",
    "        print(\" ** Epoch [%d/%d] train 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\" %\n",
    "                (epoch, n_epoch, total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch, time.time()-epoch_time))\n",
    "\n",
    "        ## save a predition of training set\n",
    "        for i in range(batch_size):\n",
    "            if np.max(b_images[i]) > 0:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch), showImage)\n",
    "                break\n",
    "            elif i == batch_size-1:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch), showImage)\n",
    "\n",
    "        ###======================== EVALUATION ==========================###\n",
    "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
    "        for batch in tl.iterate.minibatches(inputs=X_test, targets=y_test,\n",
    "                                        batch_size=batch_size, shuffle=True):\n",
    "            b_images, b_labels = batch\n",
    "            _dice, _iou, _diceh, out = sess.run([test_dice_loss,\n",
    "                    test_iou_loss, test_dice_hard, net_test.outputs],\n",
    "                    {t_image: b_images, t_seg: b_labels})\n",
    "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
    "            n_batch += 1\n",
    "\n",
    "        print(\" **\"+\" \"*17+\"test 1-dice: %f hard-dice: %f iou: %f (2d no distortion)\" %\n",
    "                (total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch))\n",
    "        print(\" task: {}\".format(task))\n",
    "        ## save a predition of test set\n",
    "        for i in range(batch_size):\n",
    "            if np.max(b_images[i]) > 0:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
    "                break\n",
    "            elif i == batch_size-1:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
    "\n",
    "        ###======================== SAVE MODEL ==========================###\n",
    "        tl.files.save_npz(net.all_params, name=save_dir+'/u_net_{}.npz'.format(task), sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
