{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gusleo/miniconda2/envs/unet/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import os, time, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorlayer==1.5.4\n",
    "#!pip install tensorflow==1.10.0\n",
    "#!conda create -n env=unet python==3.5.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "Data augmentation used to enrich original data from Brats2017. The augmentation implement to each data is below:\n",
    "1. Flip (left right)\n",
    "2. Elastic Tranform\n",
    "3. Rotate\n",
    "4. Shift\n",
    "5. Shear\n",
    "6. Zoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort_imgs(data):\n",
    "    \"\"\" data augumentation \"\"\"\n",
    "    x1, x2, x3, x4, y = data\n",
    "    # x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],  # previous without this, hard-dice=83.7\n",
    "    #                         axis=0, is_random=True) # up down\n",
    "    x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],\n",
    "                            axis=1, is_random=True) # left right\n",
    "    x1, x2, x3, x4, y = tl.prepro.elastic_transform_multi([x1, x2, x3, x4, y],\n",
    "                            alpha=720, sigma=24, is_random=True)\n",
    "    x1, x2, x3, x4, y = tl.prepro.rotation_multi([x1, x2, x3, x4, y], rg=20,\n",
    "                            is_random=True, fill_mode='constant') # nearest, constant\n",
    "    x1, x2, x3, x4, y = tl.prepro.shift_multi([x1, x2, x3, x4, y], wrg=0.10,\n",
    "                            hrg=0.10, is_random=True, fill_mode='constant')\n",
    "    x1, x2, x3, x4, y = tl.prepro.shear_multi([x1, x2, x3, x4, y], 0.05,\n",
    "                            is_random=True, fill_mode='constant')\n",
    "    x1, x2, x3, x4, y = tl.prepro.zoom_multi([x1, x2, x3, x4, y],\n",
    "                            zoom_range=[0.9, 1.1], is_random=True,\n",
    "                            fill_mode='constant')\n",
    "    return x1, x2, x3, x4, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs(X, y, path, show=False):\n",
    "    \"\"\" show one slice \"\"\"\n",
    "    if y.ndim == 2:\n",
    "        y = y[:,:,np.newaxis]\n",
    "    assert X.ndim == 3\n",
    "    tl.visualize.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
    "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
    "        X[:,:,3,np.newaxis], y]), size=(1, 5),\n",
    "        image_path=path)\n",
    "    #if(show):\n",
    "        #tl.visualize.read_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs2(X, y_, y, path, show=False):\n",
    "    \"\"\" show one slice with target \"\"\"\n",
    "    if y.ndim == 2:\n",
    "        y = y[:,:,np.newaxis]\n",
    "    if y_.ndim == 2:\n",
    "        y_ = y_[:,:,np.newaxis]\n",
    "    assert X.ndim == 3\n",
    "    tl.visualize.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
    "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
    "        X[:,:,3,np.newaxis], y_, y]), size=(1, 6),\n",
    "        image_path=path)\n",
    "    #if(show):\n",
    "        #tl.visualize.read_image(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder to save trained model and result images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] checkpoint exists ...\n",
      "[!] samples/all exists ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"all\"\n",
    "save_dir = \"checkpoint\"\n",
    "tl.files.exists_or_mkdir(save_dir)\n",
    "tl.files.exists_or_mkdir(\"samples/{}\".format(task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA\n",
    "Data from Brats2017 and loaded from prepare_data_with_valid.py\n",
    "Data importing from Brats2017 refers to DATA_SIZE define in prepate_data_with_valid which is (\"all\", \"half\", \"small\")\n",
    "1. X_train_input: contain the training data from 4 types MRI scan (flair, t1, t1-c and t2)\n",
    "2. X_train_target: contain the training data from segmentation of tumors\n",
    "3. X_dev_input: containing the validation data from 4 types MRI scan\n",
    "4. x_dev_target: containing the validation data from segementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:00<00:00, 155840.34it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival Data Count 163\n",
      "Data Size: half\n",
      "Data training used for HGG: 100 and LGG: 30\n",
      "Survival patient for HGG: 79 and LGG: 0\n",
      "LOAD ALL IMAGES' PATH AND COMPUTE MEAN/ STD\n",
      "============================\n",
      "\n",
      "Load images type : flair\n",
      "\n",
      "Load images HGG - flair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/100 [00:00<00:08, 11.21it/s]\u001b[A\n",
      "  4%|▍         | 4/100 [00:00<00:08, 11.26it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:00<00:08, 11.57it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:00<00:07, 11.97it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:00<00:07, 12.40it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:00<00:07, 12.12it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:01<00:07, 11.97it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:01<00:06, 12.12it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:01<00:06, 12.08it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:01<00:06, 11.95it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:01<00:06, 12.18it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:01<00:06, 12.49it/s]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:02<00:05, 12.49it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:02<00:06, 11.89it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:02<00:05, 11.95it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:02<00:05, 11.57it/s]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:02<00:05, 11.54it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:03<00:05, 11.57it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:03<00:05, 11.44it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:03<00:05, 11.51it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:03<00:04, 11.83it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:03<00:04, 12.23it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:03<00:04, 12.42it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:03<00:04, 12.73it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:04<00:03, 12.57it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:04<00:03, 12.20it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:04<00:03, 12.29it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:04<00:03, 12.52it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:04<00:03, 11.97it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:04<00:03, 11.58it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:05<00:03, 11.73it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:05<00:02, 12.27it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:05<00:02, 12.39it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:05<00:02, 12.53it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:05<00:02, 12.62it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:05<00:02, 12.71it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:06<00:02, 12.68it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:06<00:01, 12.81it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:06<00:01, 12.91it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:06<00:01, 13.02it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:06<00:01, 13.23it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:06<00:01, 13.25it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:06<00:01, 13.12it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [00:07<00:00, 13.13it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:07<00:00, 13.12it/s]\u001b[A\n",
      " 92%|█████████▏| 92/100 [00:07<00:00, 13.27it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:07<00:00, 13.26it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:07<00:00, 13.16it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:07<00:00, 13.25it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.41it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [00:00<00:02, 13.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images LGG - flair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:01, 13.37it/s]\u001b[A\n",
      " 20%|██        | 6/30 [00:00<00:01, 13.47it/s]\u001b[A\n",
      " 27%|██▋       | 8/30 [00:00<00:01, 13.45it/s]\u001b[A\n",
      " 33%|███▎      | 10/30 [00:00<00:01, 13.33it/s]\u001b[A\n",
      " 40%|████      | 12/30 [00:00<00:01, 13.34it/s]\u001b[A\n",
      " 47%|████▋     | 14/30 [00:01<00:01, 13.22it/s]\u001b[A\n",
      " 53%|█████▎    | 16/30 [00:01<00:01, 13.16it/s]\u001b[A\n",
      " 60%|██████    | 18/30 [00:01<00:00, 12.79it/s]\u001b[A\n",
      " 67%|██████▋   | 20/30 [00:01<00:00, 12.17it/s]\u001b[A\n",
      " 73%|███████▎  | 22/30 [00:01<00:00, 11.33it/s]\u001b[A\n",
      " 80%|████████  | 24/30 [00:01<00:00, 11.31it/s]\u001b[A\n",
      " 87%|████████▋ | 26/30 [00:02<00:00, 11.40it/s]\u001b[A\n",
      " 93%|█████████▎| 28/30 [00:02<00:00, 11.43it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:02<00:00, 12.24it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:55<02:46, 55.40s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:00<00:11,  8.65it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images type : t1\n",
      "\n",
      "Load images HGG - t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/100 [00:00<00:10,  9.65it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:00<00:08, 10.64it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:00<00:08, 11.51it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:00<00:07, 12.11it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:00<00:07, 12.44it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:00<00:06, 12.79it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:01<00:06, 13.22it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:01<00:06, 13.44it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:01<00:06, 13.16it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:01<00:05, 13.32it/s]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:01<00:05, 13.47it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:01<00:05, 13.62it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:02<00:05, 13.48it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:02<00:05, 13.61it/s]\u001b[A\n",
      " 31%|███       | 31/100 [00:02<00:05, 13.55it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:02<00:04, 13.64it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:02<00:04, 13.73it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:02<00:04, 13.86it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:02<00:04, 13.75it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:03<00:04, 13.60it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:03<00:04, 13.69it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:03<00:03, 13.76it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:03<00:03, 13.84it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:03<00:03, 13.93it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:03<00:03, 13.84it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:03<00:03, 13.92it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:04<00:03, 13.98it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:04<00:03, 13.94it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:04<00:02, 13.85it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:04<00:02, 13.70it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:04<00:02, 13.89it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:04<00:02, 13.95it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:04<00:02, 13.94it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:05<00:02, 13.83it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:05<00:02, 13.65it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:05<00:01, 13.70it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:05<00:01, 13.30it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:05<00:01, 11.69it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:05<00:01, 11.99it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:06<00:01, 12.65it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:06<00:01, 13.06it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:06<00:01, 13.21it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:06<00:00, 13.20it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:06<00:00, 13.33it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:06<00:00, 13.26it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:06<00:00, 13.12it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:07<00:00, 13.26it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [00:07<00:00, 13.48it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.47it/s][A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [00:00<00:02, 13.69it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images LGG - t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:01, 13.83it/s]\u001b[A\n",
      " 20%|██        | 6/30 [00:00<00:01, 13.96it/s]\u001b[A\n",
      " 27%|██▋       | 8/30 [00:00<00:01, 14.08it/s]\u001b[A\n",
      " 33%|███▎      | 10/30 [00:00<00:01, 14.01it/s]\u001b[A\n",
      " 40%|████      | 12/30 [00:00<00:01, 13.77it/s]\u001b[A\n",
      " 47%|████▋     | 14/30 [00:01<00:01, 13.68it/s]\u001b[A\n",
      " 53%|█████▎    | 16/30 [00:01<00:01, 13.61it/s]\u001b[A\n",
      " 60%|██████    | 18/30 [00:01<00:00, 13.61it/s]\u001b[A\n",
      " 67%|██████▋   | 20/30 [00:01<00:00, 13.78it/s]\u001b[A\n",
      " 73%|███████▎  | 22/30 [00:01<00:00, 13.45it/s]\u001b[A\n",
      " 80%|████████  | 24/30 [00:01<00:00, 13.45it/s]\u001b[A\n",
      " 87%|████████▋ | 26/30 [00:01<00:00, 13.38it/s]\u001b[A\n",
      " 93%|█████████▎| 28/30 [00:02<00:00, 13.49it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:02<00:00, 13.60it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [02:34<02:16, 68.43s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:00<00:12,  7.81it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images type : t1ce\n",
      "\n",
      "Load images HGG - t1ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 3/100 [00:00<00:10,  8.89it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:00<00:09, 10.04it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:00<00:08, 11.03it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:00<00:07, 11.71it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:00<00:07, 12.28it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:00<00:06, 12.68it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:01<00:06, 12.98it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:01<00:06, 13.37it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:01<00:06, 13.45it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:01<00:05, 13.46it/s]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:01<00:05, 13.87it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:01<00:05, 13.84it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:02<00:05, 13.68it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:02<00:05, 13.58it/s]\u001b[A\n",
      " 31%|███       | 31/100 [00:02<00:05, 13.44it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:02<00:04, 13.68it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:02<00:04, 13.75it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:02<00:04, 13.83it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:02<00:04, 13.70it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:03<00:04, 13.47it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:03<00:04, 13.42it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:03<00:04, 13.47it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:03<00:03, 13.52it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:03<00:03, 13.65it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:03<00:03, 13.65it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:03<00:03, 13.65it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:04<00:03, 13.78it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:04<00:03, 13.64it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:04<00:02, 13.74it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:04<00:02, 13.79it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:04<00:02, 13.92it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:04<00:02, 13.85it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:04<00:02, 13.81it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:05<00:02, 13.85it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:05<00:02, 13.58it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:05<00:01, 13.54it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:05<00:01, 13.70it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:05<00:01, 13.63it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:05<00:01, 13.48it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:05<00:01, 13.40it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:06<00:01, 13.44it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:06<00:01, 13.57it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:06<00:00, 13.54it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:06<00:00, 13.51it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:06<00:00, 13.43it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:06<00:00, 13.58it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:07<00:00, 13.52it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [00:07<00:00, 13.55it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.57it/s][A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [00:00<00:01, 14.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images LGG - t1ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:01, 14.22it/s]\u001b[A\n",
      " 20%|██        | 6/30 [00:00<00:01, 13.90it/s]\u001b[A\n",
      " 27%|██▋       | 8/30 [00:00<00:01, 14.11it/s]\u001b[A\n",
      " 33%|███▎      | 10/30 [00:00<00:01, 13.91it/s]\u001b[A\n",
      " 40%|████      | 12/30 [00:00<00:01, 13.87it/s]\u001b[A\n",
      " 47%|████▋     | 14/30 [00:01<00:01, 13.67it/s]\u001b[A\n",
      " 53%|█████▎    | 16/30 [00:01<00:01, 13.57it/s]\u001b[A\n",
      " 60%|██████    | 18/30 [00:01<00:00, 13.71it/s]\u001b[A\n",
      " 67%|██████▋   | 20/30 [00:01<00:00, 13.87it/s]\u001b[A\n",
      " 73%|███████▎  | 22/30 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 80%|████████  | 24/30 [00:01<00:00, 13.98it/s]\u001b[A\n",
      " 87%|████████▋ | 26/30 [00:01<00:00, 13.97it/s]\u001b[A\n",
      " 93%|█████████▎| 28/30 [00:02<00:00, 13.90it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:02<00:00, 13.83it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [03:26<01:03, 63.55s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 2/100 [00:00<00:07, 12.32it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images type : t2\n",
      "\n",
      "Load images HGG - t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 4/100 [00:00<00:07, 12.67it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:00<00:07, 12.52it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:00<00:07, 11.87it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:00<00:07, 12.05it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:00<00:07, 12.10it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:01<00:07, 12.28it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:01<00:06, 12.09it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:01<00:06, 11.91it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:01<00:06, 12.02it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:01<00:06, 12.19it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:01<00:06, 12.28it/s]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:02<00:06, 12.07it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:02<00:06, 11.92it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:02<00:05, 11.94it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:02<00:05, 11.80it/s]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:02<00:05, 11.91it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:03<00:05, 11.64it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:03<00:05, 11.54it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:03<00:05, 11.72it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:03<00:04, 11.61it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:03<00:04, 12.16it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:03<00:04, 12.11it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:04<00:04, 11.89it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:04<00:04, 11.48it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:04<00:04, 11.45it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:04<00:03, 11.53it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:04<00:03, 12.11it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:04<00:03, 11.85it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:05<00:03, 11.37it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:05<00:03, 11.45it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:05<00:03, 11.95it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:05<00:02, 12.47it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:05<00:02, 12.90it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:05<00:02, 13.08it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:05<00:02, 13.11it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:06<00:01, 13.08it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:06<00:01, 13.10it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:06<00:01, 13.44it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:06<00:01, 13.55it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:06<00:01, 13.69it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:06<00:01, 13.55it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:07<00:01, 11.43it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [00:07<00:01, 11.83it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:07<00:00, 12.12it/s]\u001b[A\n",
      " 92%|█████████▏| 92/100 [00:07<00:00, 12.28it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:07<00:00, 12.77it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:07<00:00, 12.86it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:07<00:00, 13.16it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.28it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [00:00<00:02, 13.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load images LGG - t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:01, 13.76it/s]\u001b[A\n",
      " 20%|██        | 6/30 [00:00<00:01, 13.64it/s]\u001b[A\n",
      " 27%|██▋       | 8/30 [00:00<00:01, 13.57it/s]\u001b[A\n",
      " 33%|███▎      | 10/30 [00:00<00:01, 13.69it/s]\u001b[A\n",
      " 40%|████      | 12/30 [00:00<00:01, 13.70it/s]\u001b[A\n",
      " 47%|████▋     | 14/30 [00:01<00:01, 13.79it/s]\u001b[A\n",
      " 53%|█████▎    | 16/30 [00:01<00:01, 13.72it/s]\u001b[A\n",
      " 60%|██████    | 18/30 [00:01<00:00, 13.78it/s]\u001b[A\n",
      " 67%|██████▋   | 20/30 [00:01<00:00, 13.83it/s]\u001b[A\n",
      " 73%|███████▎  | 22/30 [00:01<00:00, 13.69it/s]\u001b[A\n",
      " 80%|████████  | 24/30 [00:01<00:00, 13.67it/s]\u001b[A\n",
      " 87%|████████▋ | 26/30 [00:01<00:00, 13.74it/s]\u001b[A\n",
      " 93%|█████████▎| 28/30 [00:02<00:00, 13.79it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:02<00:00, 13.70it/s]\u001b[A\n",
      "100%|██████████| 4/4 [04:14<00:00, 63.52s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'flair': {'std': 748.459126285827, 'mean': 89.43004028208574}, 't2': {'std': 773.3985393727479, 'mean': 124.57420483354012}, 't1ce': {'std': 672.0427006176326, 'mean': 124.59248733457403}, 't1': {'std': 614.759376129325, 'mean': 109.91650340501792}}\n",
      "Preparing image for HGG Validation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:18<00:00,  1.64it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing image for LGG Validation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.52it/s]\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing image for HGG Train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing image for LGG Train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import prepare_data_with_valid as dataset\n",
    "X_train = dataset.X_train_input\n",
    "y_train = dataset.X_train_target[:,:,:,np.newaxis]\n",
    "X_test = dataset.X_dev_input\n",
    "y_test = dataset.X_dev_target[:,:,:,np.newaxis]\n",
    "\n",
    "\n",
    "if task == 'all':\n",
    "    y_train = (y_train > 0).astype(int)\n",
    "    y_test = (y_test > 0).astype(int)\n",
    "elif task == 'necrotic':\n",
    "    y_train = (y_train == 1).astype(int)\n",
    "    y_test = (y_test == 1).astype(int)\n",
    "elif task == 'edema':\n",
    "    y_train = (y_train == 2).astype(int)\n",
    "    y_test = (y_test == 2).astype(int)\n",
    "elif task == 'enhance':\n",
    "    y_train = (y_train == 4).astype(int)\n",
    "    y_test = (y_test == 4).astype(int)\n",
    "else:\n",
    "    exit(\"Unknow task %s\" % task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE HYPER-PARAMETERS\n",
    "1. Batch Size: refers to the number of training examples utilized in one iteration\n",
    "2. Learning Rate (lr): learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.\n",
    "3. Beta1: The exponential decay rate for the 1st moment estimates on Adam Optimizer \n",
    "4. Epoch: refers to the number of iteration to train all the training dataset \n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "This network using Adam Optimizer. Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "lr = 0.0001 \n",
    "# lr_decay = 0.5\n",
    "# decay_every = 100\n",
    "beta1 = 0.9\n",
    "n_epoch = 2\n",
    "print_freq_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### SHOWING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] InputLayer  u_net/inputs: (10, 240, 240, 4)\n",
      "  [TL] Conv2dLayer u_net/conv1_1: shape:[3, 3, 4, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv2_1: shape:[3, 3, 64, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv3_1: shape:[3, 3, 128, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool3: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv4_1: shape:[3, 3, 256, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool4: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv5_1: shape:[3, 3, 512, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv5_2: shape:[3, 3, 1024, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv4: shape:[3, 3, 512, 1024] out_shape:[10, 30, 30, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat4: 1024\n",
      "  [TL] Conv2dLayer u_net/uconv4_1: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv3: shape:[3, 3, 256, 512] out_shape:[10, 60, 60, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat3: 512\n",
      "  [TL] Conv2dLayer u_net/uconv3_1: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv2: shape:[3, 3, 128, 256] out_shape:[10, 120, 120, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat2: 256\n",
      "  [TL] Conv2dLayer u_net/uconv2_1: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv1: shape:[3, 3, 64, 128] out_shape:[10, 240, 240, 64] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat1: 128\n",
      "  [TL] Conv2dLayer u_net/uconv1_1: shape:[3, 3, 128, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1: shape:[1, 1, 64, 1] strides:[1, 1, 1, 1] pad:SAME act:sigmoid\n",
      "  [TL] InputLayer  u_net/inputs: (10, 240, 240, 4)\n",
      "  [TL] Conv2dLayer u_net/conv1_1: shape:[3, 3, 4, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv2_1: shape:[3, 3, 64, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv3_1: shape:[3, 3, 128, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool3: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv4_1: shape:[3, 3, 256, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] PoolLayer   u_net/pool4: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "  [TL] Conv2dLayer u_net/conv5_1: shape:[3, 3, 512, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/conv5_2: shape:[3, 3, 1024, 1024] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv4: shape:[3, 3, 512, 1024] out_shape:[10, 30, 30, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat4: 1024\n",
      "  [TL] Conv2dLayer u_net/uconv4_1: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv4_2: shape:[3, 3, 512, 512] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv3: shape:[3, 3, 256, 512] out_shape:[10, 60, 60, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat3: 512\n",
      "  [TL] Conv2dLayer u_net/uconv3_1: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv3_2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv2: shape:[3, 3, 128, 256] out_shape:[10, 120, 120, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat2: 256\n",
      "  [TL] Conv2dLayer u_net/uconv2_1: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv2_2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] DeConv2dLayer u_net/deconv1: shape:[3, 3, 64, 128] out_shape:[10, 240, 240, 64] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] ConcatLayer u_net/concat1: 128\n",
      "  [TL] Conv2dLayer u_net/uconv1_1: shape:[3, 3, 128, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1_2: shape:[3, 3, 64, 64] strides:[1, 1, 1, 1] pad:SAME act:relu\n",
      "  [TL] Conv2dLayer u_net/uconv1: shape:[1, 1, 64, 1] strides:[1, 1, 1, 1] pad:SAME act:sigmoid\n",
      "  [*] geting variables with u_net\n",
      "  got   0: u_net/conv1_1/W_conv2d:0   (3, 3, 4, 64)\n",
      "  got   1: u_net/conv1_1/b_conv2d:0   (64,)\n",
      "  got   2: u_net/conv1_2/W_conv2d:0   (3, 3, 64, 64)\n",
      "  got   3: u_net/conv1_2/b_conv2d:0   (64,)\n",
      "  got   4: u_net/conv2_1/W_conv2d:0   (3, 3, 64, 128)\n",
      "  got   5: u_net/conv2_1/b_conv2d:0   (128,)\n",
      "  got   6: u_net/conv2_2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got   7: u_net/conv2_2/b_conv2d:0   (128,)\n",
      "  got   8: u_net/conv3_1/W_conv2d:0   (3, 3, 128, 256)\n",
      "  got   9: u_net/conv3_1/b_conv2d:0   (256,)\n",
      "  got  10: u_net/conv3_2/W_conv2d:0   (3, 3, 256, 256)\n",
      "  got  11: u_net/conv3_2/b_conv2d:0   (256,)\n",
      "  got  12: u_net/conv4_1/W_conv2d:0   (3, 3, 256, 512)\n",
      "  got  13: u_net/conv4_1/b_conv2d:0   (512,)\n",
      "  got  14: u_net/conv4_2/W_conv2d:0   (3, 3, 512, 512)\n",
      "  got  15: u_net/conv4_2/b_conv2d:0   (512,)\n",
      "  got  16: u_net/conv5_1/W_conv2d:0   (3, 3, 512, 1024)\n",
      "  got  17: u_net/conv5_1/b_conv2d:0   (1024,)\n",
      "  got  18: u_net/conv5_2/W_conv2d:0   (3, 3, 1024, 1024)\n",
      "  got  19: u_net/conv5_2/b_conv2d:0   (1024,)\n",
      "  got  20: u_net/deconv4/W_deconv2d:0   (3, 3, 512, 1024)\n",
      "  got  21: u_net/deconv4/b_deconv2d:0   (512,)\n",
      "  got  22: u_net/uconv4_1/W_conv2d:0   (3, 3, 1024, 512)\n",
      "  got  23: u_net/uconv4_1/b_conv2d:0   (512,)\n",
      "  got  24: u_net/uconv4_2/W_conv2d:0   (3, 3, 512, 512)\n",
      "  got  25: u_net/uconv4_2/b_conv2d:0   (512,)\n",
      "  got  26: u_net/deconv3/W_deconv2d:0   (3, 3, 256, 512)\n",
      "  got  27: u_net/deconv3/b_deconv2d:0   (256,)\n",
      "  got  28: u_net/uconv3_1/W_conv2d:0   (3, 3, 512, 256)\n",
      "  got  29: u_net/uconv3_1/b_conv2d:0   (256,)\n",
      "  got  30: u_net/uconv3_2/W_conv2d:0   (3, 3, 256, 256)\n",
      "  got  31: u_net/uconv3_2/b_conv2d:0   (256,)\n",
      "  got  32: u_net/deconv2/W_deconv2d:0   (3, 3, 128, 256)\n",
      "  got  33: u_net/deconv2/b_deconv2d:0   (128,)\n",
      "  got  34: u_net/uconv2_1/W_conv2d:0   (3, 3, 256, 128)\n",
      "  got  35: u_net/uconv2_1/b_conv2d:0   (128,)\n",
      "  got  36: u_net/uconv2_2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got  37: u_net/uconv2_2/b_conv2d:0   (128,)\n",
      "  got  38: u_net/deconv1/W_deconv2d:0   (3, 3, 64, 128)\n",
      "  got  39: u_net/deconv1/b_deconv2d:0   (64,)\n",
      "  got  40: u_net/uconv1_1/W_conv2d:0   (3, 3, 128, 64)\n",
      "  got  41: u_net/uconv1_1/b_conv2d:0   (64,)\n",
      "  got  42: u_net/uconv1_2/W_conv2d:0   (3, 3, 64, 64)\n",
      "  got  43: u_net/uconv1_2/b_conv2d:0   (64,)\n",
      "  got  44: u_net/uconv1/W_conv2d:0   (1, 1, 64, 1)\n",
      "  got  45: u_net/uconv1/b_conv2d:0   (1,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Load checkpoint/u_net_all.npz failed!\n",
      "start training\n"
     ]
    }
   ],
   "source": [
    "showImage = True\n",
    "\n",
    "# show one slice\n",
    "X = np.asarray(X_train[80])\n",
    "y = np.asarray(y_train[80])\n",
    "# print(X.shape, X.min(), X.max()) # (240, 240, 4) -0.380588 2.62761\n",
    "# print(y.shape, y.min(), y.max()) # (240, 240, 1) 0 1\n",
    "nw, nh, nz = X.shape\n",
    "vis_imgs(X, y, 'samples/{}/_train_im.png'.format(task), showImage)\n",
    "# show data augumentation results\n",
    "for i in range(batch_size):\n",
    "    x_flair, x_t1, x_t1ce, x_t2, label = distort_imgs([X[:,:,0,np.newaxis], X[:,:,1,np.newaxis],\n",
    "            X[:,:,2,np.newaxis], X[:,:,3,np.newaxis], y])#[:,:,np.newaxis]])\n",
    "    # print(x_flair.shape, x_t1.shape, x_t1ce.shape, x_t2.shape, label.shape) # (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1)\n",
    "    X_dis = np.concatenate((x_flair, x_t1, x_t1ce, x_t2), axis=2)\n",
    "    # print(X_dis.shape, X_dis.min(), X_dis.max()) # (240, 240, 4) -0.380588233471 2.62376139209\n",
    "    vis_imgs(X_dis, label, 'samples/{}/_train_im_aug{}.png'.format(task, i), showImage)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    with tf.device('/gpu:0'): #<- remove it if you train on CPU or other GPU\n",
    "        ###======================== DEFIINE MODEL =======================###\n",
    "        ## nz is 4 as we input all Flair, T1, T1c and T2.\n",
    "        t_image = tf.placeholder('float32', [batch_size, nw, nh, nz], name='input_image')\n",
    "        ## labels are either 0 or 1\n",
    "        t_seg = tf.placeholder('float32', [batch_size, nw, nh, 1], name='target_segment')\n",
    "        ## train inference\n",
    "        net = model.u_net(t_image, is_train=True, reuse=False, n_out=1)\n",
    "        ## test inference\n",
    "        net_test = model.u_net(t_image, is_train=False, reuse=True, n_out=1)\n",
    "\n",
    "        ###======================== DEFINE LOSS =========================###\n",
    "        ## train losses\n",
    "        out_seg = net.outputs\n",
    "        dice_loss = 1 - tl.cost.dice_coe(out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
    "        iou_loss = tl.cost.iou_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
    "        dice_hard = tl.cost.dice_hard_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
    "        loss = dice_loss\n",
    "\n",
    "        ## test losses\n",
    "        test_out_seg = net_test.outputs\n",
    "        test_dice_loss = 1 - tl.cost.dice_coe(test_out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
    "        test_iou_loss = tl.cost.iou_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
    "        test_dice_hard = tl.cost.dice_hard_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
    "\n",
    "    ###======================== DEFINE TRAIN OPTS =======================###\n",
    "    t_vars = tl.layers.get_variables_with_name('u_net', True, True)\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope('learning_rate'):\n",
    "            lr_v = tf.Variable(lr, trainable=False)\n",
    "        train_op = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(loss, var_list=t_vars)\n",
    "\n",
    "    ###======================== LOAD MODEL ==============================###\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    ## load existing model if possible\n",
    "    tl.files.load_and_assign_npz(sess=sess, name=save_dir+'/u_net_{}.npz'.format(task), network=net)\n",
    "    print(\"start training\")\n",
    "    ###======================== TRAINING ================================###\n",
    "    for epoch in range(0, n_epoch+1):\n",
    "        epoch_time = time.time()\n",
    "        ## update decay learning rate at the beginning of a epoch\n",
    "        # if epoch !=0 and (epoch % decay_every == 0):\n",
    "        #     new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "        #     sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
    "        #     log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "        #     print(log)\n",
    "        # elif epoch == 0:\n",
    "        #     sess.run(tf.assign(lr_v, lr))\n",
    "        #     log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "        #     print(log)\n",
    "\n",
    "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
    "        for batch in tl.iterate.minibatches(inputs=X_train, targets=y_train,\n",
    "                                    batch_size=batch_size, shuffle=True):\n",
    "            images, labels = batch\n",
    "            step_time = time.time()\n",
    "            ## data augumentation for a batch of Flair, T1, T1c, T2 images\n",
    "            # and label maps synchronously.\n",
    "            data = tl.prepro.threading_data([_ for _ in zip(images[:,:,:,0, np.newaxis],\n",
    "                    images[:,:,:,1, np.newaxis], images[:,:,:,2, np.newaxis],\n",
    "                    images[:,:,:,3, np.newaxis], labels)],\n",
    "                    fn=distort_imgs) # (10, 5, 240, 240, 1)\n",
    "            b_images = data[:,0:4,:,:,:]  # (10, 4, 240, 240, 1)\n",
    "            b_labels = data[:,4,:,:,:]\n",
    "            b_images = b_images.transpose((0,2,3,1,4))\n",
    "            b_images.shape = (batch_size, nw, nh, nz)\n",
    "\n",
    "            ## update network\n",
    "            _, _dice, _iou, _diceh, out = sess.run([train_op,\n",
    "                    dice_loss, iou_loss, dice_hard, net.outputs],\n",
    "                    {t_image: b_images, t_seg: b_labels})\n",
    "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
    "            n_batch += 1\n",
    "\n",
    "            ## you can show the predition here:\n",
    "            # vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_tmp.png\".format(task))\n",
    "            # exit()\n",
    "\n",
    "            # if _dice == 1: # DEBUG\n",
    "            #     print(\"DEBUG\")\n",
    "            #     vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_debug.png\".format(task))\n",
    "\n",
    "            if n_batch % print_freq_step == 0:\n",
    "                print(\"Epoch %d step %d 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\"\n",
    "                % (epoch, n_batch, _dice, _diceh, _iou, time.time()-step_time))\n",
    "\n",
    "            ## check model fail\n",
    "            if np.isnan(_dice):\n",
    "                exit(\" ** NaN loss found during training, stop training\")\n",
    "            if np.isnan(out).any():\n",
    "                exit(\" ** NaN found in output images during training, stop training\")\n",
    "\n",
    "        print(\" ** Epoch [%d/%d] train 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\" %\n",
    "                (epoch, n_epoch, total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch, time.time()-epoch_time))\n",
    "\n",
    "        ## save a predition of training set\n",
    "        for i in range(batch_size):\n",
    "            if np.max(b_images[i]) > 0:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch), showImage)\n",
    "                break\n",
    "            elif i == batch_size-1:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch), showImage)\n",
    "\n",
    "        ###======================== EVALUATION ==========================###\n",
    "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
    "        for batch in tl.iterate.minibatches(inputs=X_test, targets=y_test,\n",
    "                                        batch_size=batch_size, shuffle=True):\n",
    "            b_images, b_labels = batch\n",
    "            _dice, _iou, _diceh, out = sess.run([test_dice_loss,\n",
    "                    test_iou_loss, test_dice_hard, net_test.outputs],\n",
    "                    {t_image: b_images, t_seg: b_labels})\n",
    "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
    "            n_batch += 1\n",
    "\n",
    "        print(\" **\"+\" \"*17+\"test 1-dice: %f hard-dice: %f iou: %f (2d no distortion)\" %\n",
    "                (total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch))\n",
    "        print(\" task: {}\".format(task))\n",
    "        ## save a predition of test set\n",
    "        for i in range(batch_size):\n",
    "            if np.max(b_images[i]) > 0:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
    "                break\n",
    "            elif i == batch_size-1:\n",
    "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
    "\n",
    "        ###======================== SAVE MODEL ==========================###\n",
    "        tl.files.save_npz(net.all_params, name=save_dir+'/u_net_{}.npz'.format(task), sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
